{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"046156a2fad9435b84a19ff3163e7e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"087abc8baba848babf93b8f29e5a2bcf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"196ef60df0cd417f934f6e2304c3f180":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfb40ab3aa994fc9b3088d79ed5c4a26","placeholder":"‚Äã","style":"IPY_MODEL_046156a2fad9435b84a19ff3163e7e4a","value":"Generating‚Äátest‚Äásplit:‚Äá100%"}},"1a74e168c9554fac9978a4736dbcdb11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bc12b432fe5444c8e854153cf84120d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d7dc3fd42e04783b7e912161ec5f7c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ed0f7a88aab4e7f9e7c3216bea48b90","IPY_MODEL_48276a93bae44504aa0a485b207051d0","IPY_MODEL_8466623dbeb442468367747c2b187cdd"],"layout":"IPY_MODEL_2b60182eb6724c0fa3d241a39854cd1e"}},"23ec6113e4014445baf3dcc4dd6c4e17":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27456772617447dc80c58eb292c185c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_196ef60df0cd417f934f6e2304c3f180","IPY_MODEL_9113790223204b179dfeff0623f0136d","IPY_MODEL_d84c3a99025742ae939a2472f36852fa"],"layout":"IPY_MODEL_daed4150f70b4cd1b913f26a732b13c5"}},"28b385c6e67d4c05a4277889c9f5c0c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b60182eb6724c0fa3d241a39854cd1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed0f7a88aab4e7f9e7c3216bea48b90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28b385c6e67d4c05a4277889c9f5c0c4","placeholder":"‚Äã","style":"IPY_MODEL_1bc12b432fe5444c8e854153cf84120d","value":"Generating‚Äátrain‚Äásplit:‚Äá100%"}},"3374e62bec23487098102668768cc9ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41ba45cbf8a74dbebe9b62f5b321d315":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48276a93bae44504aa0a485b207051d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3374e62bec23487098102668768cc9ef","max":68686,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce09826095904bc18c8eaaff388a216d","value":68686}},"5a0a4e972dfd49a0a87b274ec3fd97e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dff51ffe23724245a70b220217d5b891","placeholder":"‚Äã","style":"IPY_MODEL_d29e7cfee3fe449e82569927b355dab0","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"695b0f6ecfa143efa7c9c4e22cc33b47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"696d7cf5fbb64bc484d7de8bc66a4062":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"751d396294e74f9490817610756c4a01":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8466623dbeb442468367747c2b187cdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9947797e53bf41c9a93c9b94c877c863","placeholder":"‚Äã","style":"IPY_MODEL_de403b0aaea3409996b04e0c826bc71a","value":"‚Äá68686/68686‚Äá[00:01&lt;00:00,‚Äá48807.63‚Äáexamples/s]"}},"9113790223204b179dfeff0623f0136d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23ec6113e4014445baf3dcc4dd6c4e17","max":7632,"min":0,"orientation":"horizontal","style":"IPY_MODEL_696d7cf5fbb64bc484d7de8bc66a4062","value":7632}},"9947797e53bf41c9a93c9b94c877c863":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb5f95f83dce49d49478528866903599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9dd2d8eef0f437b8e0d2b385e2220bd","placeholder":"‚Äã","style":"IPY_MODEL_695b0f6ecfa143efa7c9c4e22cc33b47","value":"‚Äá2/2‚Äá[00:28&lt;00:00,‚Äá13.72s/it]"}},"c44003cd636b48a486e85c6dbe57177e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ba45cbf8a74dbebe9b62f5b321d315","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_751d396294e74f9490817610756c4a01","value":2}},"c48ecf14955a4d80bd5c8b8c2c7da38d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a0a4e972dfd49a0a87b274ec3fd97e2","IPY_MODEL_c44003cd636b48a486e85c6dbe57177e","IPY_MODEL_bb5f95f83dce49d49478528866903599"],"layout":"IPY_MODEL_e48f4e809eee461cb458e20d1fd73847"}},"ce09826095904bc18c8eaaff388a216d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfb40ab3aa994fc9b3088d79ed5c4a26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d29e7cfee3fe449e82569927b355dab0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d84c3a99025742ae939a2472f36852fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_087abc8baba848babf93b8f29e5a2bcf","placeholder":"‚Äã","style":"IPY_MODEL_1a74e168c9554fac9978a4736dbcdb11","value":"‚Äá7632/7632‚Äá[00:00&lt;00:00,‚Äá32020.60‚Äáexamples/s]"}},"daed4150f70b4cd1b913f26a732b13c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de403b0aaea3409996b04e0c826bc71a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dff51ffe23724245a70b220217d5b891":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e48f4e809eee461cb458e20d1fd73847":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9dd2d8eef0f437b8e0d2b385e2220bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"state":{}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13419490,"sourceType":"datasetVersion","datasetId":8516716},{"sourceId":13477774,"sourceType":"datasetVersion","datasetId":8556530}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installation","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers # For Qwen3VLForConditionalGeneration","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show torch torchaudio torchvision transformers bitsandbytes trl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Unsloth\n\n- Load The Base Model\n- Note: Unsloth's `FastVisonModel` Only Supports Training On Single GPU. Loading And LoRA Might Work Fine, Training Start Will Cause Error On Multi-GPU.\n- So `device_map=\"auto/balanced\"` Will Not Work Good","metadata":{}},{"cell_type":"code","source":"from unsloth import FastVisionModel # FastLanguageModel for LLMs\nimport torch\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"Qwen/Qwen3-VL-2B-Instruct\", # OR \"Qwen/Qwen3-VL-2B-Thinking\" (Try Others Like InternVL)\n    load_in_4bit = False, \n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["c48ecf14955a4d80bd5c8b8c2c7da38d","5a0a4e972dfd49a0a87b274ec3fd97e2","c44003cd636b48a486e85c6dbe57177e","bb5f95f83dce49d49478528866903599","e48f4e809eee461cb458e20d1fd73847","dff51ffe23724245a70b220217d5b891","d29e7cfee3fe449e82569927b355dab0","41ba45cbf8a74dbebe9b62f5b321d315","751d396294e74f9490817610756c4a01","e9dd2d8eef0f437b8e0d2b385e2220bd","695b0f6ecfa143efa7c9c4e22cc33b47"]},"id":"QmUBVEnvCDJv","outputId":"0a396f9a-e514-4650-9c9b-3ad3a9369816","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Apply LoRA\n\n- Recommended To Set R and Alpha Equal\n\n**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!","metadata":{"id":"SXd9bTZd1aaL"}},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers     = False, # False if not finetuning vision layers\n    finetune_language_layers   = True, # False if not finetuning language layers\n    finetune_attention_modules = True, # False if not finetuning attention layers\n    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n\n    r = 256,           # The larger, the higher the accuracy, but might overfit\n    lora_alpha = 256,  # Recommended alpha == r at least\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n)","metadata":{"id":"6bZsfBuZDeCL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Prep\n**We'll Be Using a text dataset in Instruction-Tuning SFT Format Converted In Suitable Format For `FastVisionModel`**\n\n**Below We Just Verify By Inspecting Few Examples From Converted Dataset. And Make Train Split**\n\n\n#### For Conversion Guide All You Need Is A JSON Dataset In Alpaca Format\n**See [https://github.com/Vinayyyy7/FineTune-Vision-LMs.git](http://)**\n\n**Your Output Should Be :**\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"How to finetune a VL Model Only On Text-Based Dataset\"\n        }\n      ]\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"For This Check Out This Notebook + Conversion Of Your Normal Alpaca Format Dataset To Vision SFT Dataset\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n- Excluded `type: \"image\"` For Only Training It's Brain Not Eyes.","metadata":{"id":"vITh0KVJ10qX"}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('json', data_files=\"path/to/data.jsonl\", split = \"train\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["1d7dc3fd42e04783b7e912161ec5f7c2","2ed0f7a88aab4e7f9e7c3216bea48b90","48276a93bae44504aa0a485b207051d0","8466623dbeb442468367747c2b187cdd","2b60182eb6724c0fa3d241a39854cd1e","28b385c6e67d4c05a4277889c9f5c0c4","1bc12b432fe5444c8e854153cf84120d","3374e62bec23487098102668768cc9ef","ce09826095904bc18c8eaaff388a216d","9947797e53bf41c9a93c9b94c877c863","de403b0aaea3409996b04e0c826bc71a","27456772617447dc80c58eb292c185c8","196ef60df0cd417f934f6e2304c3f180","9113790223204b179dfeff0623f0136d","d84c3a99025742ae939a2472f36852fa","daed4150f70b4cd1b913f26a732b13c5","cfb40ab3aa994fc9b3088d79ed5c4a26","046156a2fad9435b84a19ff3163e7e4a","23ec6113e4014445baf3dcc4dd6c4e17","696d7cf5fbb64bc484d7de8bc66a4062","087abc8baba848babf93b8f29e5a2bcf","1a74e168c9554fac9978a4736dbcdb11"]},"id":"LjY75GoYUCB8","outputId":"121963f3-8e3f-4c7f-e4a5-36c1ddc12a54","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfcSGwIb6p_R","outputId":"43de1949-d15c-4956-d57d-d641f0671816","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[2][\"messages\"]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"VTzhtzNRAEL1","outputId":"6f12a3ee-9f95-40a2-e3f8-e7af5d4e4c3c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train the model\n\n**We'll Be Using Transformer's DataCollator For Finetuning A Vision Model On A Text Only Dataset**\n\n### Steps \n\n- First We Load Our Vision Model's Tokenizer And Our Converted Dataset\n- Format The Dataset Entires\n- Tokenize The Dataset To Feed Into The Model\n- Set Training Args\n- Start Fresh OR Resume From A Checkpoint","metadata":{"id":"idAEIeSQ3xdS"}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import DataCollatorForSeq2Seq, AutoTokenizer\n\n# Load tokenizer\nbase_tokenizer = AutoTokenizer.from_pretrained(\n    \"Qwen/Qwen3-VL-2B-Instruct\",\n    trust_remote_code=True\n)\n\nif base_tokenizer.pad_token is None:\n    base_tokenizer.pad_token = base_tokenizer.eos_token\n\n# Load JSONL directly\ndataset = load_dataset(\"json\", data_files=\"/path/to/data.jsonl\")\ntrain_dataset = dataset[\"train\"]\n\nprint(f\"Loaded {len(train_dataset)} samples\")\nprint(f\"Sample:\\n{train_dataset[0]}\")\n\n# Preprocessing function\ndef format_messages(examples):\n    \"\"\"Format messages as text\"\"\"\n    formatted_texts = []\n    \n    for messages in examples['messages']:\n        text = \"\"\n        for msg in messages:\n            role = msg.get('role', '').upper()\n            content = msg.get('content', [])\n            \n            msg_text = \"\"\n            for item in content:\n                if isinstance(item, dict) and item.get('type') == 'text':\n                    msg_text += item.get('text', '')\n            \n            text += f\"{role}: {msg_text}\\n\"\n        \n        formatted_texts.append(text.strip())\n    \n    tokenized = base_tokenizer(\n        formatted_texts,\n        truncation=True,\n        max_length=2048,\n        padding=\"max_length\",\n        return_tensors=None\n    )\n    \n    tokenized[\"labels\"] = [ids[:] for ids in tokenized[\"input_ids\"]]\n    \n    return tokenized\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(\n    format_messages,\n    batched=True,\n    batch_size=16,\n    remove_columns=['messages'],\n    desc=\"Formatting and tokenizing\"\n)\n\nprint(f\"Preprocessed dataset ready!\")\n\nFastVisionModel.for_training(model)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=base_tokenizer,\n    data_collator=DataCollatorForSeq2Seq(base_tokenizer, pad_to_multiple_of=8),\n    train_dataset=train_dataset,\n    args=SFTConfig(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        # max_steps=30,\n        num_train_epochs=1,\n        learning_rate=2e-5,\n        logging_steps=1, # For Each Step\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        save_total_limit=2, # To Save Only 2 Latest Checkpoints\n        seed=3407,\n        output_dir=\"/kaggle/working/checkpoints\",\n        report_to=\"none\",\n        remove_unused_columns=False,\n        max_length=2048,\n    ),\n)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95_Nn-89DhsL","outputId":"c4a76c64-06c7-4aa8-f2fd-038653804e6f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- START TRAINING FROM FRESH","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- RESUME FROM A EXISITING CHECKPOINT IN `/kaggle/checkpoints` e.g `/kaggle/checkpoints/checkpoint-3000`","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train(resume_from_checkpoint = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save FineTuned LoRA Adapters For Your VL Model","metadata":{"id":"uMuVrWbjAzhc"}},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")  # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upcOlWe7A1vc","outputId":"61cf35a0-c007-45ce-f760-8f8f5d82d661","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Now Load The FineTuned LoRA With Your Loaded Base Model","metadata":{"id":"AEEcJ4qfC7Lp"}},{"cell_type":"code","source":"if True:\n    from unsloth import FastVisionModel\n    model, tokenizer = FastVisionModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        load_in_4bit = True, # Set to False for 16bit LoRA\n    )\n    FastVisionModel.for_inference(model) # Enable for inference!\n\nimage = \"path/to/image.jpg\"\ninstruction = \"Tell Me What Can You See In This Picture.\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\"},\n        {\"type\": \"text\", \"text\": instruction}\n    ]}\n]\ninput_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\ninputs = tokenizer(\n    image,\n    input_text,\n    add_special_tokens = False,\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n                   use_cache = True, temperature = 1.5, min_p = 0.1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKX_XKs_BNZR","outputId":"edc73b33-f7cf-4351-9c81-8a06162d59f3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Merge The Full Finetuned Model In 16bit","metadata":{"id":"f422JgM9sdVT"}},{"cell_type":"code","source":"# Save locally to 16bit\nif True: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)","metadata":{"id":"iHjt_SMYsd3P","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Upload To Huggingface\n\n- Upload Checkpoints\n- Upload Fully Merged Model\n- Upload Just LoRA Adapters","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Upload Any Of The Folders From Above After Login","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import upload_folder\nimport os\n\nfolder_path = input(\"Enter the full path of the folder to upload (e.g., /kaggle/checkpoints/checkpoint-100): \").strip()\n\nif not os.path.exists(folder_path):\n    raise FileNotFoundError(f\"‚ùå Folder not found: {folder_path}\")\nif not os.path.isdir(folder_path):\n    raise NotADirectoryError(f\"‚ùå This is not a folder: {folder_path}\")\n\n# Input repo ID\nrepo_id = input(\"Enter the Hugging Face repo ID (e.g., username/my-model): \").strip()\nif not repo_id:\n    raise ValueError(\"Repo ID cannot be empty.\")\n\nlocal_folder_name = os.path.basename(folder_path)\nrepo_subfolder = input(f\"Enter subfolder name in repo (default: '{local_folder_name}'): \").strip()\nif not repo_subfolder:\n    repo_subfolder = local_folder_name\n\n# Perform upload using stored credentials\nprint(f\"\\nüöÄ Uploading '{folder_path}' to repo '{repo_id}', subfolder: '{repo_subfolder}'...\")\n\ntry:\n    upload_folder(\n        folder_path=folder_path,\n        repo_id=repo_id,\n        repo_type=\"model\",           # Use \"dataset\" if uploading to a dataset repo\n        path_in_repo=repo_subfolder,\n        commit_message=f\"Upload folder {local_folder_name}\"\n    )\n    print(f\"\\n‚úÖ Successfully uploaded to: https://huggingface.co/{repo_id}/tree/main/{repo_subfolder}\")\nexcept Exception as e:\n    print(f\"‚ùå Upload failed: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Download And Save Any Checkpoint To Continue Training","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download, snapshot_download\nfrom pathlib import Path\nimport os\n\nrepo_id = input(\"Enter the Hugging Face repo ID (e.g., username/model-name): \").strip()\nif not repo_id:\n    raise ValueError(\"Repo ID cannot be empty.\")\n\nprint(\"\\nChoose download mode:\")\nprint(\"1 ‚Üí Download entire repo\")\nprint(\"2 ‚Üí Download a specific folder only\")\nchoice = input(\"\\nEnter 1 or 2: \").strip()\n\nlocal_dir = input(f\"\\nEnter the local path to save files (e.g., /kaggle/working/downloaded): \").strip()\nif not local_dir:\n    local_dir = \"/kaggle/working\"  # fallback\n\nos.makedirs(local_dir, exist_ok=True)\nprint(f\"üìÅ Files will be saved to: {os.path.abspath(local_dir)}\")\n\ntry:\n    if choice == \"1\":\n        # Download entire repo\n        print(f\"\\nüöÄ Downloading entire repo '{repo_id}'...\")\n        snapshot_download(\n            repo_id=repo_id,\n            repo_type=\"model\",  # Change to \"dataset\" if needed\n            local_dir=local_dir,\n            local_dir_use_symlinks=False  # Copies all files fully (no symlinks)\n        )\n        print(f\"\\n‚úÖ Entire repo downloaded to: {os.path.abspath(local_dir)}\")\n\n    elif choice == \"2\":\n        # Download specific folder\n        folder_name = input(\"Enter the exact folder name to download (e.g., checkpoint-100): \").strip()\n        if not folder_name:\n            raise ValueError(\"Folder name cannot be empty.\")\n\n        print(f\"\\nüöÄ Downloading folder '{folder_name}' from repo '{repo_id}'...\")\n\n        # We use snapshot_download with allow_patterns to filter just that folder\n        snapshot_download(\n            repo_id=repo_id,\n            repo_type=\"model\", \n            local_dir=local_dir,\n            local_dir_use_symlinks=False,\n            allow_patterns=f\"{folder_name}/**\"  # Include all subfiles\n        )\n        print(f\"\\n‚úÖ Folder '{folder_name}' downloaded to: {os.path.join(os.path.abspath(local_dir), folder_name)}\")\n\n    else:\n        print(\"‚ùå Invalid choice. Please run again and enter 1 or 2.\")\n\nexcept Exception as e:\n    print(f\"‚ùå Download failed: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Thank You Now You Have A Fully Finetuned Merged Vision Language Model With Your Own Preferences ! Ready To Chat","metadata":{}}]}
